--Author--

Onur Yuksel. MSc Thesis Uppsala University, Computational Science Department, 2023.

###

--Abstract--

On the highest heights of the third AI spring where the interest in AI research and industrial applications is booming, it’s worth taking a step back to reexplore the basics. With the spirit of data-centric-ai, we discuss the use of Cellular Automata as a resource for synthetic training data and explore how properties of CA rules relate to learning. We showcase three experiments with Neural Networks (NN) where we utilize 1-d elementary CA as training data. First, we explore how well a 1-hidden-layered NN learns the rules of CA. Second, we look at how the lambda property -of a rule- relates to the network's uncertainty. And finally, we explore how the CA rule itself affects the network's uncertainty, in connection to the number of parameters in the network and the number of training data points. While the results are qualitative and require elaboration, the findings from the last experiment can be summarized. We point out to three behavioural patterns of 1-hidden-layered networks: (I) networks that exhibit convergence to zero uncertainty; (II) networks that struggle at non-zero uncertainty values with low variance throughout the experiment parameter space; (III) networks that exhibit high uncertainty variance.

###

--Introduction--

Growing interest in the use of synthetic data for studying machine learning methods inspired us to use Cellular Automata (CA) for training Neural Networks (NN) to investigate how well they learn the rules of CA. In this master's thesis, we present (I) our approach of treating CA as a resource for synthetic training data and (II) results from experiments showing how well one-hidden-layered neural networks learn to become different kinds of 1-d cellular automata. 

Currently, many NN architectures are available through popular Python libraries like Tensorflow and PyTorch, not to mention the popular model sharing platform Hugging Face. New architectural approaches to NNs are also being experimented for general or application specific purposes. The availability of a large selection of methods, pretrained models and detailed discussions of the network architectures are valuable resources for AI research and application development purposes. Yet, when evaluating a new NN architecture or observing a NN's performance `in the wild', discussions of the qualities of training data have not been the greatest focus. The popular approach to qualities of training data has rather been `the more data the better'. The more data is better in terms of capturing a greater distribution of the phenomenon, yet, a more sophisticated approach can benefit the AI research community. Uses of synthetic data for training networks where underlying patterns or properties of data is known, can help simplify the problem of `explaining the training data' and shed further light on what or how NNs learn.

Synthetic training data would be useful for (I) evaluating a new NN architecture or a new ML algorithm and (II) revealing any unknowns about NNs, as an addition to making experiments with data collected `in the wild'. Having more control on the properties of training data would allow controlled experiments where the mechanics of NN is static and a property of training data is changed. The learning performance of the NN could then be investigated in relation to these properties that researchers choose to study.

In this study, we performed two series of experiments. First, we created a 12 neuron, one-hidden-layered NN and looked at how this NN learns different 1-d CA rules, under the same CA configuration. Second, we performed the previous experiment altering the number of neurons of the NN and the number of data points in the training data. The first experiment is designed only to look at the uncertainty of the NN when trained with different CA rules whereas the second experiment reveals the importance of number of data points versus the number of neurons, for the training data that is generated by each CA rule.

There could be different approaches on how to feed CA inputs (X) and outputs (Y) to a NN. In this study, we perform a random sampling process of X and Y pairs from time steps of CA. The sampling is made under the assumption that the exact Xs -cells- that is related to an output -cell- Y is known. This way, the NN is provided with the direct causal link between Xs and Ys, and therefore its learning capability is only up to how well the training data represents the diversity of X and Y pairs of the CA. Alternatively, a time series interpretation of the CA as training data can be another approach for when building on the approach of this study in the future. 

###

--Background--

The concept of Cellular Automata (CA) dates back to the 60s, the earliest examples being introduced by Von Neumann. CA can be considered as a popular phenomenon, the reasons that led to popularity of the concept could be that they are easy to configure and they exhibit a variety of behavior. Wolfram discusses CA in relation to dynamic systems theory and the formal theory of computation and argues that CA are of sufficient generality to provide simple models for a variety of physical, chemical, biological systems. Here, the self-organizing properties that can be found in natural phenomena, i.e., outlines of snowflakes, patterns of flow in turbulent fluids, are particularly of interest in relation to CA. In a similar fashion, Langton argues that CA can be considered as formal abstractions of physical systems. In this study, the primary reason why we used CA as training data does not relate to the discussion around CA regarding generalizability to self-organization behavior in natural processes or physical systems. We are primarily interested in CA, from a pure data-centric perspective, where the properties of a CA -system- are exposed and can be varied.

In this work, we are mainly interested in CA in terms of their continuous ability to produce their K^N state distribution, as we treat CA as a source of training data. There are certain studies that touch upon CA behavior that are relevant to what we are interested in. These are the lambda parameter and classification of elementary CA rules.

In the article Universality and complexity in cellular automata, Wolfram presents the four classes of elementary CA behaviour. Wolfram’s classes are as follows; Class I, CA evolves to a homogeneous state; Class II, CA evolves periodically;  Class III, CA evolves chaotically; Class IV, CA shows complex behaviour that involves all previous classes.

There hasn't been extensive use of CA in terms of making experiments in a machine learning context. There are though certain interesting studies that touch upon the approach in our study. Recently, Gilpin has shown that 2-d CA rules can be learnt by convolutional neural networks. An interesting angle of the results here is the suggestion that the entropy of a physical process can affect its representation when learned by neural networks, i.e., the final loss is correlated with the entropy of the rule. Due to the difference of the mechanics of our experiment to Gilpin's study, we do not try to compare our results, but the investigation of the entropy of the rule to the loss creates value in terms of a data-centric research perspective.

###

--Implementation of a CA Framework--

An elementary cellular automata framework that treats the rules according to Wolfram Code was implemented in Python. The current version of the framework consists of binary states, i.e., K=2 and periodic boundary conditions were employed. The software has three components: A Cellular Automata component for atomic CA operations; a Cells component for collecting time steps of CA outputs and an Image Utility component for saving the history as an image.

Cellular Automata component has the following functionalities.

Given an integer rule -Wolfram Code-, a radius r, and current n states, return the next n state for each 2r+1 sites using periodic boundary conditions.
From an input of length n, allow generating an output of length n, as well as an array of explicit mappings of each 2r+1 length input site to a length-1 output. This functionality is practical for our sampling process.
Allow injection of a parameter to the CA function, omega, that represents a probability that the output of the CA is flipped, i.e., allow a level of stochasticity.

###

--Training Data Simulations--

In the way of exploring CA as an easy-to-configure medium for generating training data scenarios, we first make use of the rules of CA. All rules constitute a dynamical system with a different behaviour, except the equivalence in the rule space. We utilize all rules of radius-1 CA and sample a number of rules when we look at radius-2 CA. Configuring time steps, sampling mechanics and stochasticity, we create four simulations for each rule we look at. 

###

--Four scenarios--

For an initial verification that NN mechanics are able to learn the rules under perfect scenario, we created (I) Rules Provided Simulation, the first training simulation that generates the training data where all X and Y mappings of the rules are provided to the NN. (II) Initial Conditions Provided Simulation is created with 10 restarts and 70 time steps with random initial conditions. Here, initial time steps and any transients are available for observation. (III) Evolved Behavior Simulation is created with 10 restarts and 140 time steps with random initial conditions. 70 first time steps are omitted from the start. The idea with this is to simulate a case where a dynamical system is being observed but the initial states are unavailable. The resulting characteristics of the training data here is interesting to compare to the Initial Conditions Provided Simulation. Finally, (IV) Evolved and Stochastic Simulation is created with the parameters of the Evolved Simulation but with adding stochasticity to the rules where the level of stochasticity -ω- is 0.05. In the simulations except the Rules Provided Simulation, the default configuration is that the size of the CA cells are 30 and 3 randomly sampled data points are generated from each time step. This means that 2100 data points are used in training. For further clarity, a list of selected parameters of the simulations can be found on Appendix B under the details of each experiment. We modify these configurations based on the characteristics of the experiments that are introduced further on.

###

--Experiment Design--

It wouldn’t come as a surprise to the reader if the author claims that one could design a neural network architecture that is able to learn the 2r + 1 to 1 input output mappings of an elementary CA. There are though a number of ways to design a neural network for this problem. In this study, the input channels (i.e., the 2r + 1 input sites) are treated as categorical features and the output is designed as a binary classification problem. The trained model of this kind of network can be used to create two different types of CA depending on the experimental need. It can be considered (I) as a deterministic cellular automaton if the update rules would be based on the maximum of softmax posterior probabilities that the model returns. Or alternatively, if the update rules of the trained model would be based on a stochastic decision process according to weighted posterior probabilities between the binary classes, then we obtain (II) a probabilistic cellular automata. In our experiments, we are only interested in the uncertainty measure of the network, using the softmax posteriors. First, to verify that single layer NNs are able to learn the rules of CA, an initial experiment is performed with training data that consists of all the possible input-output combinations.

###

--Initial experiment under perfect training data--

An initial training round for all 256 rules of elementary CA of r = 1 and K = 2 is performed with single layer neural networks. In this scenario, networks are presented with all 8 possible inputs and outputs as training data and asked to predict them after. The entire training data for rule 0 and rule 110 that the network is trained with in this round can be seen below. Here, the CA is reinitialized with a different initial condition corresponding to integer 0 to 7 after one time step, therefore the vertical reading of the training data would not correspond to a continuous time series of the CA rules, except for each t and t+1 where t is even. When we do a sampling from the training data we take care of the restarted initial conditions so that the NN is not presented with the new initial condition as an output to the last time step of the previous run.

###

A network has more learning capability when we increase the number of parameters it has. For the particular problem of learning the CA rules from training data presented in 9, a hidden layer with a single neuron would also perform well. Though we start with a neuron size that would be the highest that we would like to incorporate in our experiments overall. For an initial insight on how well the -single layer, 12 neuron- network learns, the posterior probabilities of rules 0, 40, 10, 30 and 110 for all 8 input cases are shown below. Resulting categorical accuracy values of all cases are observed to be 1.

###

Taking a closer look at the results, a neural network trained to be a Rule 0 CA seems to give the most accurate posteriors among the rules that are examined; for all inputs i the probability of the next state (Pi0) being 0 is observed as 0.999. Since Rule 0 results 0 for all inputs, the neural network more easily approximates to a deterministic CA. The reader should note that the neural network can be further optimized i.e., with a lower learning rate, for all rules to converge to a better posterior, under the perfect training data conditions. With these results observed, it can be confirmed that given the training data with all possible inputs, the network is able to learn to become an elementary CA. Now, the experiment starts. Keeping the NN and all its hyperparameters constant, the network can be tested in terms of its uncertainty under each simulation. Further, we continue the experiment with varying neuron size and number of data points for a more rigorous capture of the NN uncertainty behaviour.

###

--Varying CA rule, static NN--

Keeping the number of parameters of the network on the higher side would be an easy way to make sure that lack of parameters is not affecting the learning negatively, as we are interested in looking at how the training data that is generated by each radius-1 CA rule affects learning, in a controlled experiment procedure. The NN is intended to be constant throughout the experiment and for each rule, a new training data is generated and presented to a newly initialized NN with the following features: an input Layer that treats states of a CA site as a categorical feature; 12-unit Dense Layer with ReLU activation; 2-unit output layer with Softmax activation. A visual representation of the NN is shown below.

As per optimisation algorithm, loss function and accuracy metric, following choices are made: Adam optimiser with default learning rate 0.001; Sparse categorical cross entropy as loss: Sparse categorical accuracy as an accuracy metric. Moreover, the network is set to train for batch size 40 and epochs number 1500. A detailed list of parameters that are used to configure this experiment can be found on appendix figure 25.

###

--Uncertainty of NN vs lambda--

Even though the lambda parameter is not able to capture the behavioral qualities of CA perfectly, it is worth investigating this parameter against uncertainty of the NNs. For this, we choose to sample training data under the Evolved Behavior Simulation. For radius-1 CA we keep a row size of 30 cells and for radius-2 CA we increase row size to 60 cells. This is to increase the expressive capability of the simulation for radius-2 CA. For the radius-1 case, we keep the data point multiplier 1 and for the radius-2 case, we look at data point multiplier 3 and 9 to investigate if sampling more points changes anything. This investigation can be considered a follow up to our Varying CA rule, static neural network experiment, so we keep the NNs configuration same as the previous experiment, i.e., neuron count as 12 on the hidden layer. The parameters that are used to configure this experiment can be found on appendix figure 26. We are also interested in looking at how parameter size and the size of data points affect the learning. Therefore, we designed a final experiment where the number of neurons and data point size are both varied.

###

--Varying number of neurons and data point size--

For having a more rigorous experimental control, we vary the neuron size on the hidden layer from 1 to 11. This would allow us to observe how parameter size is related to learning. Moreover, we also change the number data points that are sampled. We configure a parameter for sampling, namely data point multiplier, that defines how many X and Ys are sampled from each row. This parameter is varied from 1 to 9 for each experiment round. Following figure shows the varying NN architectures used in this experiment.

As per optimisation algorithm, loss function and accuracy metric, following choices are made: Adam optimiser with default learning rate 0.01; Sparse categorical cross entropy as loss; Sparse categorical accuracy as an accuracy metric. For each of the 256 CA rules, the experiment is run 99 times for each data point size and number of neurons. Here, compared to the previous experiment, the learning rate and epoch are decreased to 0.01 and 150 respectively. The parameters that are used to configure this experiment can be found on appendix figure 27.

###

--Experiment Results--

For the results we refer to the thesis fulltext. But the results are summarized in conclusion section.

###

--Conclusion--

We present an exploratory study to motivate the use of Cellular Automata as a resource for synthesizing training data. Theoretical or practical studies can potentially benefit from synthesizing training data from CA as they are easy to configure and the variety of rules increase exponentially by increasing its radius and number of values that a cell can take. As a dynamical system -medium- that has been around since the 60s, it is exciting to imagine the ways that modern machine learning studies can make use of CA. We present three simulations. First, we expose initial time steps to the training data with Initial Conditions Provided simulation. Second, we skip a number of time steps to create Evolved Behaviour simulation. And third, we add stochasticity to CA, skip a number of time steps to create Evolved and Stochastic simulation. With a neural network with static configuration, we look at how well the network learns the rules of radius-1 CA, also in connection with Wolfram’s classification of CA rules. We find out that chaotic and complex behaviour exhibiting CA are easier to learn. Further, our focus is only on the Evolved Behaviour simulation. Using radius-1 and radius-2 CA, we look at if the lambda property -of a rule- can show any relationship to the network’s uncertainty. Even though lambda cannot cluster the network uncertainty well, we find out that around λ ≈ 0.5, a number of rules caused network to have 0 uncertainty. Lastly, we look at the uncertainty of the network in connection to its parameter size and number of data points sampled. As we generate new training data -from radius-1 CAwhen we vary each parameter, the experiment can be considered as an inquiry to group behavioural patterns of 1-hidden-layered networks, where the effect of variations in the training data -different scenarios within the same rule- are also present. We point out to three groups: first, networks that exhibit convergence to zero uncertainty; second, networks that struggle at non-zero uncertainty values with low variance throughout the parameter space; and third, networks that exhibit high uncertainty variance.


